{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "\n",
    "url = \"https://db.netkeiba.com/race/202405050511\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\"}\n",
    "\n",
    "request = Request(url, headers=headers)\n",
    "html = urlopen(request).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_html(html)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 開催日一覧を取得\n",
    "```\n",
    "https://race.netkeiba.com/top/calendar.html?pid=race_calendar&year=2023\n",
    "https://race.netkeiba.com/top/race_list.html?kaisai_date=20230105\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://race.netkeiba.com/top/calendar.html?pid=race_calendar&year=2023\"\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\"}\n",
    "request = Request(url, headers=headers)\n",
    "html = urlopen(request).read()\n",
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# htmlから要素を取り出したい時はBeutifulsoupをつかう \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html)\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aタグのhref要素から開催日を取り出す\n",
    "a_tag = soup.find(\"table\", class_=\"Calendar_Table\").find(\"a\")\n",
    "a_tag[\"href\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 開催日を抽出 \n",
    "import re\n",
    "re.findall(r\"kaisai_date=(\\d{8})\", a_tag[\"href\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_tag_list = soup.find(\"table\", class_=\"Calendar_Table\").find_all(\"a\")\n",
    "# kaisai_date_list = []\n",
    "# for a_tag in a_tag_list:\n",
    "#   kaisai_date = re.findall(r\"kaisai_date=(\\d{8})\", a_tag[\"href\"])[0]\n",
    "#   kaisai_date_list.append(kaisai_date)\n",
    "\n",
    "# kaisai_date_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_open_with_headers(url):\n",
    "  headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\"}\n",
    "  request = Request(url, headers=headers)\n",
    "  html = urlopen(request).read()\n",
    "  return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def scrape_kaisai_date(from_, to_):\n",
    "  kaisai_date_list = []\n",
    "  for date in tqdm(pd.date_range(from_, to_, freq=\"MS\")):\n",
    "    year, month = date.year, date.month\n",
    "    url = f\"https://race.netkeiba.com/top/calendar.html?pid=race_calendar&year={year}&month={month}\"\n",
    "    html = url_open_with_headers(url)\n",
    "    time.sleep(1) # 忘れない\n",
    "    soup = BeautifulSoup(html)\n",
    "    a_tag_list = soup.find(\"table\", class_=\"Calendar_Table\").find_all(\"a\")\n",
    "    \n",
    "    for a_tag in a_tag_list:\n",
    "      kaisai_date = re.findall(r\"kaisai_date=(\\d{8})\", a_tag[\"href\"])[0]\n",
    "      kaisai_date_list.append(kaisai_date)\n",
    "\n",
    "  return kaisai_date_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from_, to_ = \"2023-01\", \"2023-12\"\n",
    "\n",
    "# kaisai_date_list_at_2023 = scrape_kaisai_date(\"2023-01\", \"2023-12\")\n",
    "# kaisai_date_list_at_2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 開催ページからIDを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://race.netkeiba.com/top/race_list.html?kaisai_date=20230105\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = url_open_with_headers(url)\n",
    "soup = BeautifulSoup(html, features=\"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"div\", class_=\"RaceList_Box\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取得できないときはselenium, Chrome Driverを使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install selenium\n",
    "# pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chromeのversionにあったdriverを準備する→chromeを使う前提\n",
    "driver_path = ChromeDriverManager().install()\n",
    "driver = webdriver.Chrome(service=Service(driver_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "li_list = driver.find_elements(By.CLASS_NAME, \"RaceList_DataItem\")\n",
    "li_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li_0 = li_list[0]\n",
    "li_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "href = li_0.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "rece_id = re.findall(r\"race_id=(\\d{12})\", href)[0]\n",
    "rece_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_id_list = []\n",
    "for li in li_list:\n",
    "  href = li.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "  race_id = re.findall(r\"race_id=(\\d{12})\" ,href)[0]\n",
    "  race_id_list.append(race_id)\n",
    "\n",
    "race_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(race_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scraping import scrape_kaisai_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_date_list = scrape_kaisai_date(from_=\"2023-01\", to_=\"2023-12\")\n",
    "race_date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "\n",
    "def scrape_race_ids(race_date_list: list[str]):\n",
    "  driver_options = Options()\n",
    "  driver_options.add_argument(\"--headless\") # ヘッドレスで実行\n",
    "  driver_path = ChromeDriverManager().install()\n",
    "  race_id_list = []\n",
    "  with webdriver.Chrome(service=Service(driver_path), options=driver_options) as driver:\n",
    "  # driver = webdriver.Chrome(service=Service(driver_path), options=driver_options)\n",
    "    for race_date in tqdm(race_date_list):\n",
    "      url = f\"https://race.netkeiba.com/top/race_list.html?kaisai_date={race_date}\"\n",
    "      try:\n",
    "        driver.get(url)\n",
    "        time.sleep(1) # 忘れない\n",
    "        li_list = driver.find_elements(By.CLASS_NAME, \"RaceList_DataItem\")\n",
    "        \n",
    "        for li in li_list:\n",
    "          href = li.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "          race_id = re.findall(r\"race_id=(\\d{12})\" ,href)[0]\n",
    "          race_id_list.append(race_id)\n",
    "      except:\n",
    "        print(\"stop at\", url)\n",
    "        print(traceback.format_exc())\n",
    "        break\n",
    "    \n",
    "  # driver.quit()\n",
    "  return race_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scraping import scrape_kaisai_date\n",
    "race_date_list = race_date_list = scrape_kaisai_date(from_=\"2023-01\", to_=\"2023-02\")\n",
    "race_id_list = scrape_race_ids(race_date_list)\n",
    "\n",
    "race_id_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### レース詳細画面からテーブルを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scraping import url_open_with_headers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://db.netkeiba.com/race/202306010101\"\n",
    "html = url_open_with_headers(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.read_html(html)\n",
    "table[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"src/race_id_list_2023.pkl\", \"rb\") as f:\n",
    "  race_id_list = pickle.load(f)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(race_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# for race_id in race_id_list[:4]:\n",
    "#   url = f\"https://db.netkeiba.com/race/{race_id}\"\n",
    "#   html = url_open_with_headers(url)\n",
    "#   time.sleep(1)\n",
    "#   df = pd.read_html(html)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`df`を`dump`するの前に、後々、データ追加したくなる時用にHTML自体を保存する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/html/202306010101.bin\", \"wb\") as f:\n",
    "  f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "HTML_DIR = Path(\"data\", \"html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(HTML_DIR / \"202306010101.bin\", \"wb\") as f:\n",
    "  f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "HTML_DIR = Path(\"data\", \"html\")\n",
    "\n",
    "def scrape_race_detail_html(race_id_list: list[str], save_dir: Path) -> list[Path]:\n",
    "  html_path_list = []\n",
    "  for race_id in tqdm(race_id_list):\n",
    "    # file exists check\n",
    "    filepath = save_dir / f\"{race_id}.bin\"\n",
    "    if filepath.is_file():\n",
    "      print(\"skipped\", filepath)\n",
    "      continue\n",
    "    \n",
    "    # scraping\n",
    "    url = f\"https://db.netkeiba.com/race/{race_id}\"\n",
    "    html = url_open_with_headers(url)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # write html\n",
    "    with open(filepath, \"wb\") as f:\n",
    "      f.write(html)\n",
    "    html_path_list.append(filepath)\n",
    "  \n",
    "  return html_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_path_list = scrape_race_detail_html(race_id_list[:12], HTML_DIR)\n",
    "html_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.constants import PROJECT_ROOT, RACE_DETAIL_HTML_DIR\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(RACE_DETAIL_HTML_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_path_list = list(RACE_DETAIL_HTML_DIR.glob(\"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(html_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "for html_path in html_path_list[:3]:\n",
    "  with open(html_path, \"rb\") as f:\n",
    "    race_id = html_path.stem\n",
    "    html = f.read()\n",
    "    df = pd.read_html(html)[0]\n",
    "    df.index = [race_id] * len(df) # race_idをindexに(list * int = 繰り返しになる)\n",
    "    dfs[race_id] = df\n",
    "    \n",
    "# valueだけ繋げる\n",
    "concat_df = pd.concat(dfs.values())\n",
    "concat_df.index.name = \"race_id\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results(html_path_list: list[Path]) -> pd.DataFrame:\n",
    "  dfs = {}\n",
    "  for html_path in tqdm(html_path_list):\n",
    "    with open(html_path, \"rb\") as f:\n",
    "      race_id = html_path.stem\n",
    "      html = f.read()\n",
    "      df = pd.read_html(html)[0]\n",
    "      df.index = [race_id] * len(df) # race_idをindexに(list * int = 繰り返しになる)\n",
    "      dfs[race_id] = df\n",
    "    \n",
    "  # valueだけ繋げる\n",
    "  concat_df = pd.concat(dfs.values())\n",
    "  concat_df.index.name = \"race_id\"\n",
    "  return concat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = create_results(html_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ウマの過去成績テーブルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ウマの過去成績ページ\n",
    "url = \"https://db.netkeiba.com/horse/2020103575\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scraping import url_open_with_headers\n",
    "\n",
    "html = url_open_with_headers(url)\n",
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_html(html)[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### raceに紐づくウマIDを取得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 対象レースのURL\n",
    "race_url = \"https://db.netkeiba.com/race/202306010101/\"\n",
    "html = url_open_with_headers(race_url)\n",
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "table = soup.find(\"table\", class_=\"race_table_01 nk_tb_common\")\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### aタグの中でhorseを含むものだけにを絞り込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# ^は前方一致\n",
    "a_list = table.find_all(\"a\", href=re.compile(r\"^/horse/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URLからrace idを取り出す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_url = a_list[0][\"href\"]\n",
    "horse_id = re.findall(r\"\\d{10}\", example_url)[0]\n",
    "horse_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "horse_id_list = []\n",
    "for a in tqdm(a_list):\n",
    "  horse_id = re.findall(r\"\\d{10}\", a[\"href\"])[0]\n",
    "  horse_id_list.append(horse_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(horse_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 関数化テスト\n",
    "from src.create_rawdf import create_race_results\n",
    "from src.constants import RACE_DETAIL_HTML_DIR, RACE_RESULTS_RAWDF_DIR\n",
    "\n",
    "html_path_list_2023 = list(RACE_DETAIL_HTML_DIR.glob(\"*.bin\"))\n",
    "results = create_race_results(html_path_list=html_path_list_2023[:10], \n",
    "                         save_dir=RACE_RESULTS_RAWDF_DIR, \n",
    "                         save_filename=\"results_2023.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resultsを保存しておく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 関数に組み込む\n",
    "# from src.constants import RACE_RESULTS_RAWDF_DIR\n",
    "\n",
    "# results.to_csv(RACE_RESULTS_RAWDF_DIR / \"results_2023.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ウマID一覧を取得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horse_id_list = results[\"horse_id\"].unique()\n",
    "len(horse_id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 馬の過去成績テーブルを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.constants import HORSE_DETAIL_HTML_DIR\n",
    "from src.scraping import scrape_horse_detail_html\n",
    "\n",
    "html_path_horse = scrape_horse_detail_html(horse_id_list=horse_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.constants import HORSE_DETAIL_HTML_DIR, RACE_RESULTS_RAWDF_DIR\n",
    "html_path_horse = list(HORSE_DETAIL_HTML_DIR.glob(\"*.bin\"))\n",
    "len(html_path_horse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.create_rawdf import create_horse_results\n",
    "\n",
    "horse_results = create_horse_results(html_path_list=html_path_horse, save_dir=RACE_RESULTS_RAWDF_DIR, save_filename=\"horse_results_2023\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
